# Table of Contents
* Abstract
* [Introduction](#1-introduction)
* [Related Work](#2-related-work)
* [Technical Approach](#3-technical-approach)
* [Evaluation and Results](#4-evaluation-and-results)
* [Discussion and Conclusions](#5-discussion-and-conclusions)
* [References](#6-references)

# Abstract

In the expansive domain of autonomous driving, where vehicles often operate in isolation from human intervention, our innovation strives to reintroduce human interaction – but from a perspective not previously explored. Instead of being limited within the cabin, we imagine a world where drivers can step out and control their vehicles from the outside. This proposal is advantageous for narrow parking situations or navigating congested areas. While adept at capturing gestures, vision-only systems often have difficulty achieving the precision needed for car controls, especially in environments with variable lighting or obstructions. On the other hand, while promising accuracy, wearable-only solutions suffer from their continuous signal transmission, which can blur the boundaries of intentional commands. We aim to capitalize on the gesture recognition of the former and the precision of the latter by fusing vision with wearables.

# 1. Introduction

This section should cover the following items:

* Motivation & Objective: The objective of this project was to transform the way we interact with autonomous vehicles. We aimed to enable drivers to control their vehicles from outside, using gestures. This approach was born from the desire to make urban driving, especially in tight spaces like narrow parking and congested areas, more manageable and intuitive.

* State of the Art & Its Limitations: Currently, autonomous driving technologies rely heavily on in-car controls and automated systems. While recent advancements in vision-based gesture control are impressive, they offer only discrete control options, limiting their practicality in complex driving scenarios. Our project sought to overcome this by integrating vision-based systems with wearable technology, allowing for more nuanced, continuous control — akin to using a hand as a mouse to navigate more precisely.

* Novelty & Rationale: The innovative aspect of our project lies in this seamless integration of gesture recognition with wearable tech. This combination enables continuous, fluid control commands, a significant leap from the binary commands typical in current systems. We believe this approach will succeed because it aligns with natural human movements, making it more intuitive and effective for users.

* Potential Impact: 

* Challenges: The main challenges include developing a highly accurate gesture recognition system that works reliably in varied environments and seamlessly integrating this system with different vehicle models. Additionally, ensuring user comfort and acceptance of the wearable technology is crucial.

* Requirements for Success: What skills and resources are necessary to perform the project?

* Metrics of Success: Success would be measured by the system's accuracy in gesture recognition, its adaptability to various vehicle models, user satisfaction, and its effectiveness in improving navigation in tight spaces. Additionally, exploring the extension of this method for other control applications, like using hand gestures as a mouse, would indicate the versatility and potential for broader impact of the technology.

# 2. Related Work

# 3. Technical Approach

# 4. Evaluation and Results

# 5. Discussion and Conclusions

# 6. References

