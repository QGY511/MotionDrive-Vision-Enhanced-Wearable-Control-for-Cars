# Project Proposal

## 1. Motivation & Objective

In an era of technological marvels, we stand at the cusp of redefining vehicular autonomy. Our vision is to create a symphony of interaction where the vehicle is not just a passive entity but an extension of the human will. We envisage a future where drivers, liberated from the confines of their seats, can command their vehicles externally with the finesse of a conductor leading an orchestra. This capability is especially transformative in tight parking scenarios and navigating through congested urban landscapes, melding the precision of technology with the intuition of human control.

## 2. State of the Art & Its Limitations

Currently, autonomous driving largely relies on two predominant technologies: computer vision and wearable tech. Vision-based systems, though adept at wide-range gesture recognition, falter in precision, particularly in variable lighting and obstructed view scenarios. Wearables, while offering accuracy, struggle with continuous signal transmission that often misinterprets user intent. These limitations underscore the need for a more integrated and reliable approach.

## 3. Novelty & Rationale

Our project introduces a groundbreaking hybrid model, seamlessly fusing the extensive gesture recognition capabilities of computer vision with the pinpoint accuracy of wearable technology. By interpreting hand movements as dynamic, continuous sequences, our system initiates with broad gesture recognition and refines control through wearable feedback, ensuring unparalleled precision in vehicular manipulation.

## 4. Potential Impact

The successful realization of this project will be a leap forward in autonomous vehicle technology. It will not only enhance the practicality of vehicle control in tight spaces but also redefine the interaction between humans and machines, making autonomous vehicles more adaptable and intuitive. The broader implications extend to improved road safety, efficient traffic management, and an overall enrichment of the driving experience.

## 5. Challenges

The project's primary challenges include achieving flawless integration of computer vision and wearables, developing sophisticated algorithms for seamless gesture interpretation, and ensuring robust performance across diverse environmental conditions. Additionally, ensuring user-friendly interfaces and safeguarding against potential security vulnerabilities are critical.


## 6. Requirements for Success

Success hinges on a multi-disciplinary approach combining expertise in machine learning, computer vision, sensor technology, and user interface design. Essential resources include advanced computer vision systems, accelerometers, and a robust software development environment. Collaboration across these specialties will be key to overcoming the technical complexities of this project.

## 7. Metrics of Success

We will measure success through several key indicators: accuracy of gesture recognition, precision in vehicular control, user satisfaction, and adaptability to varying environmental contexts. Additionally, system responsiveness, reliability, and ease of use will be critical metrics.

## 8. Execution Plan

The project will progress through phases of design, development, testing, and refinement. Initial stages will focus on developing the integrated system, followed by extensive testing in controlled environments. Feedback loops will be essential for iterative improvement. Team roles will be allocated based on expertise in computer vision, wearable technology, software development, and user experience design.

## 9. Related Work

### 9.a. Papers

The methodology and findings from this study provide crucial insights into the feasibility and practicality of using hand gestures for nuanced vehicle control, thereby contributing valuable knowledge to our endeavor of enhancing human-vehicle interaction in autonomous driving systems.

### 9.b. Datasets

Gesture Recognition: datasets from google mediapipe package are used.
Waerable: gryo sensor data are also recorded from our testing.

### 9.c. Software

Python for vision processing and C++ for gyro data processing.

## 10. References

https://edgeimpulse.com
HCI in Business, Government and Organizations: 9th International Conference, HCIBGO 2022, Held as Part of the 24th HCI International Conference, HCII 2022, Virtual Event, June 26 – July 1, 2022, ProceedingsJun 2022Pages 247–260
https://link.springer.com/chapter/10.1007/978-3-031-05544-7_19

